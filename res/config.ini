[CRAWLER]
# http://localhost/internet/website1/page1.html     https://www.google.com/search?client=ubuntu&channel=fs&q=lauch&ie=utf-8&oe=utf-8
# https://de.wikipedia.org/wiki/Lauch
# https://www.google.com/search?client=ubuntu&channel=fs&q=lauch&ie=utf-8&oe=utf-8
# https://web.archive.org/web/20080916124519/http://www.dmoz.org/
startURL: https://de.wikipedia.org/wiki/Lauch
# reproductionPerPage ^ reproductionRate = running crawler threads
reproductionRate: 2
reproductionPerPage: 2
hopCount: 10
# retries if no urls found
retries = 1


[QUEUE]
# overwrite or append dump file
overwrite: True
directDump: True
dumpFile: res/url_dump.csv
