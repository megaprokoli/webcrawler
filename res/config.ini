[CRAWLER]
# http://localhost/internet/website1/page1.html
# https://de.wikipedia.org/wiki/Lauch
# https://www.google.com/search?client=ubuntu&channel=fs&q=lauch&ie=utf-8&oe=utf-8
# https://web.archive.org/web/20080916124519/http://www.dmoz.org/
# https://www.flickr.com/explore
# https://www.google.de/search?hl=de&tbm=isch&source=hp&biw=1855&bih=885&ei=swLeW-TzGPHRrgSy4r64Bw&q=far+cry+5&oq=far+cry&gs_l=img.3.0.0l10.11446.29328.0.30918.14.8.3.3.3.0.167.937.2j6.8.0....0...1ac.1.64.img..0.14.1007....0.NPhUk6_JSDI
startURL: https://web.archive.org/web/20080916124519/http://www.dmoz.org/
# reproductionPerPage ^ reproductionRate = running crawler threads
reproductionRate: 2
reproductionPerPage: 2
hopCount: 10
# retries if no urls found
retries = 1


[QUEUE]
# overwrite or append dump file
overwrite: True
directDump: True
dumpFile: res/url_dump.csv


[WORKER]
type: img
# 0 = auto mode
# number should be smaller than the amount of urls. recommended: thread_number = url_amount * 0.0005
threads: 0
